{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-grams_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryblue31/JavaBlue/blob/master/N_grams_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ0UEjxIqmvN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGDbCwESt1yB"
      },
      "source": [
        "#1st implementation\n",
        "\n",
        "\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "\n",
        "#read text from file\n",
        "def read_file(file):\n",
        "    f = open(file, \"r\")\n",
        "    text = f.read()\n",
        "    return text\n",
        "\n",
        "text = read_file(\"train_data.txt\")\n",
        "text = text.lower()\n",
        "\n",
        "#split text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "sentences_tokenized = [];\n",
        "\n",
        "#tokenize sentences\n",
        "tweet_wt = TweetTokenizer()\n",
        "\n",
        "for sent in sentences:\n",
        "    sent_tok = tweet_wt.tokenize(sent);\n",
        "    sentences_tokenized.append(sent_tok);\n",
        "\n",
        "#tokenize text \n",
        "tokens = tweet_wt.tokenize(text);\n",
        "\n",
        "#find word frequencies\n",
        "count = nltk.FreqDist(tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#set vocabulary (distinct words with frequency >10)\n",
        "vocabulary = set()\n",
        "for key in tokens:\n",
        "    if(tokens.count(key)>=10):\n",
        "        vocabulary.add(key);\n",
        "\n",
        "vocab_size = len(vocabulary)+1 #1 is the unknown token\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#replace the unknown words in the training set with the token unknown(replace only in sentences_tokenized)\n",
        "for sent in sentences_tokenized:\n",
        "    for i in range(len(sent)):\n",
        "        if(sent[i] not in vocabulary):\n",
        "            sent[i] = 'UNK'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#hash tables with ngrams frequencies\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in sentences_tokenized:\n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent,2,pad_left=True, pad_right=True,left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent,3,pad_left=True, pad_right=True,left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# cross entropy and perplexity\n",
        "\n",
        "alpha = 0.01;\n",
        "\n",
        "\n",
        "test_text = read_file(\"test_data.txt\")\n",
        "test_text = test_text.lower()\n",
        "\n",
        "sentences_of_test = sent_tokenize(test_text)\n",
        "sentences_tokenized_of_test = [];\n",
        "\n",
        "for sent in sentences_of_test:\n",
        "    sent_tok = tweet_wt.tokenize(sent);\n",
        "    sentences_tokenized_of_test.append(sent_tok);\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    for i in range(len(sent)):\n",
        "        if(sent[i] not in vocabulary):\n",
        "            sent[i] = 'UNK'\n",
        "\n",
        "#for bigram model\n",
        "sum_prob = 0\n",
        "bigram_cnt = 0\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    sent = ['<s>'] + sent + ['<e>']\n",
        "\n",
        "    # Iterate over the bigrams of the sentence\n",
        "    for idx in range(2, len(sent)):\n",
        "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] + alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(bigram_prob)\n",
        "        bigram_cnt += 1\n",
        "\n",
        "HC = -sum_prob / bigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy of bigram model: {0:.3f}\".format(HC))\n",
        "print(\"Perplexity of bigram model: {0:.3f}\".format(perpl))\n",
        "\n",
        "#for trigram model\n",
        "sum_prob = 0\n",
        "trigram_cnt = 0\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n",
        "\n",
        "    # Iterate over the trigrams of the sentence\n",
        "    for idx in range(4,len(sent) - 1):\n",
        "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] + alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(trigram_prob)\n",
        "        trigram_cnt+=1\n",
        "\n",
        "HC = -sum_prob / trigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy of trigram model: {0:.3f}\".format(HC))\n",
        "print(\"Perplexity of trigram model: {0:.3f}\".format(perpl))\n",
        "\n",
        "\n",
        "# Beam Search\n",
        "\n",
        "\n",
        "#find bigram probability\n",
        "def findBigramPropability(start, end):\n",
        "    bigram_prop = (bigram_counter[(start, end)] + alpha) / (unigram_counter[(start,)] + alpha*vocab_size)\n",
        "    bigram_prop = math.log2(bigram_prop);\n",
        "    return bigram_prop\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# algorithm Levenshtein\n",
        "\n",
        "def call_counter(func):\n",
        "    def helper(*args, **kwargs):\n",
        "        helper.calls += 1\n",
        "        return func(*args, **kwargs)\n",
        "    helper.calls = 0\n",
        "    helper.__name__= func.__name__\n",
        "\n",
        "    return helper\n",
        "memo = {}\n",
        "@call_counter\n",
        "def levenshtein(s, t):\n",
        "    if s == \"\":\n",
        "        return len(t)\n",
        "    if t == \"\":\n",
        "        return len(s)\n",
        "    cost = 0 if s[-1] == t[-1] else 1\n",
        "       \n",
        "    i1 = (s[:-1], t)\n",
        "    if not i1 in memo:\n",
        "        memo[i1] = levenshtein(*i1)\n",
        "    i2 = (s, t[:-1])\n",
        "    if not i2 in memo:\n",
        "        memo[i2] = levenshtein(*i2)\n",
        "    i3 = (s[:-1], t[:-1])\n",
        "    if not i3 in memo:\n",
        "        memo[i3] = levenshtein(*i3)\n",
        "    res = min([memo[i1]+1, memo[i2]+1, memo[i3]+cost])\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dictionary with nearest words\n",
        "\n",
        "def min_dist_word(token, beamwidth = 10 ):\n",
        "    mydict = {}\n",
        "    for i in range(0,beamwidth):\n",
        "        mydict[i] = 500 \n",
        "\n",
        "    for word in vocabulary:\n",
        "        d = levenshtein(token,word)\n",
        "        key_max = max(mydict.keys(), key=(lambda k: mydict[k]))\n",
        "        \n",
        "        if d < mydict[key_max]:\n",
        "            mydict.pop(key_max)\n",
        "            mydict[word] = d\n",
        "           \n",
        "            \n",
        "\n",
        "    for key in mydict:\n",
        "        mydict[key] = math.log2(1/(mydict[key] + 1))\n",
        "\n",
        "    return mydict # returns for each token, one dictionary with { word : 1/(dist +1) }\n",
        "\n",
        "\n",
        "\n",
        "# Spelling corrector    \n",
        "\n",
        "def findPath(sequence,beamwidth = 10):\n",
        "    l1 = 0.4\n",
        "    l2 = 0.6\n",
        "\n",
        "    tokens = tweet_wt.tokenize(sequence);\n",
        "    \n",
        "    tokens = ['<s>'] + tokens + ['<e>']\n",
        "\n",
        "    path1 = [('<s>',1)]\n",
        "    path2 = [('<s>',1)]\n",
        "\n",
        "    for i in range(1,len(tokens)-1):\n",
        "        \n",
        "        next_word_dict = min_dist_word(tokens[i], beamwidth)\n",
        "        \n",
        "        list = []\n",
        "        for word in next_word_dict:\n",
        "            \n",
        "            \n",
        "            p = l2*next_word_dict[word] + l1*findBigramPropability(path1[i-1][0], word )\n",
        "           \n",
        "            word_prob = [p,word,'path1']\n",
        "            list.append(word_prob)\n",
        "\n",
        "            p = l2*next_word_dict[word] + l1* findBigramPropability(path2[i-1][0],word )\n",
        "            \n",
        "            word_prob = [p,word,'path2']\n",
        "            list.append(word_prob)\n",
        "\n",
        "        w1 = list.index(max(list))\n",
        "        w1 = list.pop(w1)\n",
        "        word1 = w1[1]\n",
        "      \n",
        "        \n",
        "\n",
        "        w2 = list.index(max(list))\n",
        "        w2 = list.pop(w2)\n",
        "        word2 = w2[1]\n",
        "\n",
        "        \n",
        "\n",
        "        if (w1[2] == 'path1' and w2[2] == 'path1'):\n",
        "          path2 = path1\n",
        "          path1.append([word1, w1[0]])\n",
        "          path2.append([word2, w2[0]])\n",
        "        elif (w1[2] == 'path2' and w2[2] == 'path2'):\n",
        "          path1 = path2\n",
        "          path1.append([word1, w1[0]])\n",
        "          path2.append([word2, w2[0]])\n",
        "        elif (w1[2] == 'path1' and w2[2] == 'path2' ):\n",
        "          path1.append([word1,w1[0]])\n",
        "          path2.append([word2,w2[0]])\n",
        "        else:\n",
        "          path1.append([word2, w2[0]])\n",
        "          path2.append([word1, w1[0]])\n",
        "\n",
        "\n",
        "\n",
        "        if i == len(tokens) - 2 :    # when we reach the end of the sentence\n",
        "            sum1 = 0\n",
        "            sum2 = 0\n",
        "\n",
        "            \n",
        "\n",
        "            for i in range(len(path1)):\n",
        "                sum1 += path1[i][1]\n",
        "            \n",
        "\n",
        "            for i in range(len(path2)):\n",
        "                sum2 += path2[i][1]\n",
        "            \n",
        "\n",
        "            if (sum1 > sum2):\n",
        "                string = ''\n",
        "                for i in range(1,len(path1)):\n",
        "                    string = string + path1[i][0] + ' '\n",
        "                return string\n",
        "\n",
        "            else:\n",
        "                string = ''\n",
        "                for i in range(1,len(path2)):\n",
        "                    string = string + path2[i][0] + ' '\n",
        "                return string\n",
        "\n",
        "\n",
        "\n",
        "sentence = input('Give a sentence')   #test with input sentence\n",
        "print(findPath(sentence))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKWrHfqeI2mX"
      },
      "source": [
        "#2nd implementation\n",
        "\n",
        "\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "\n",
        "#read text from file\n",
        "def read_file(file):\n",
        "    f = open(file, \"r\")\n",
        "    text = f.read()\n",
        "    return text\n",
        "\n",
        "text = read_file(\"train_data.txt\");\n",
        "text = text.lower()\n",
        "\n",
        "#split text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "sentences_tokenized = [];\n",
        "\n",
        "#tokenize sentences\n",
        "tweet_wt = TweetTokenizer()\n",
        "\n",
        "for sent in sentences:\n",
        "    sent_tok = tweet_wt.tokenize(sent);\n",
        "    sentences_tokenized.append(sent_tok);\n",
        "\n",
        "#tokenize text \n",
        "tokens = tweet_wt.tokenize(text);\n",
        "\n",
        "#find word frequencies\n",
        "count = nltk.FreqDist(tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#set vocabulary (distinct words with frequency >10)\n",
        "vocabulary = set()\n",
        "for key in tokens:\n",
        "    if(tokens.count(key)>=10):\n",
        "        vocabulary.add(key);\n",
        "\n",
        "vocab_size = len(vocabulary)+1 #1 is the unknown token\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#replace the unknown words in the training set with the token unknown(replace only in sentences_tokenized)\n",
        "for sent in sentences_tokenized:\n",
        "    for i in range(len(sent)):\n",
        "        if(sent[i] not in vocabulary):\n",
        "            sent[i] = 'UNK'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#hash tables with ngrams frequencies\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in sentences_tokenized:\n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent,2,pad_left=True, pad_right=True,left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent,3,pad_left=True, pad_right=True,left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# cross entropy and perplexity\n",
        "\n",
        "alpha = 0.01;\n",
        "\n",
        "\n",
        "test_text = read_file(\"test_data.txt\")\n",
        "test_text = test_text.lower()\n",
        "\n",
        "sentences_of_test = sent_tokenize(test_text)\n",
        "sentences_tokenized_of_test = [];\n",
        "\n",
        "for sent in sentences_of_test:\n",
        "    sent_tok = tweet_wt.tokenize(sent);\n",
        "    sentences_tokenized_of_test.append(sent_tok);\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    for i in range(len(sent)):\n",
        "        if(sent[i] not in vocabulary):\n",
        "            sent[i] = 'UNK'\n",
        "\n",
        "#for bigram model\n",
        "sum_prob = 0\n",
        "bigram_cnt = 0\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    sent = ['<s>'] + sent + ['<e>']\n",
        "\n",
        "    # Iterate over the bigrmas of the sentence\n",
        "    for idx in range(2, len(sent)):\n",
        "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] + alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(bigram_prob)\n",
        "        bigram_cnt += 1\n",
        "\n",
        "HC = -sum_prob / bigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy of bigram model: {0:.3f}\".format(HC))\n",
        "print(\"Perplexity of bigram model: {0:.3f}\".format(perpl))\n",
        "\n",
        "#for trigram model\n",
        "sum_prob = 0\n",
        "trigram_cnt = 0\n",
        "\n",
        "for sent in sentences_tokenized_of_test:\n",
        "    sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n",
        "\n",
        "    # Iterate over the trigrams of the sentence\n",
        "    for idx in range(4,len(sent) - 1):\n",
        "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] + alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
        "        sum_prob += math.log2(trigram_prob)\n",
        "        trigram_cnt+=1\n",
        "\n",
        "HC = -sum_prob / trigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy of trigram model: {0:.3f}\".format(HC))\n",
        "print(\"Perplexity of trigram model: {0:.3f}\".format(perpl))\n",
        "\n",
        "\n",
        "\n",
        "# Beam Search\n",
        "\n",
        "#find bigram probability\n",
        "def findBigramPropability(start, end):\n",
        "    bigram_prop = (bigram_counter[(start, end)] + alpha) / (unigram_counter[(start,)] + alpha*vocab_size)\n",
        "    bigram_prop = math.log2(bigram_prop);\n",
        "    return bigram_prop\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# algorithm Levenshtein \n",
        "\n",
        "def call_counter(func):\n",
        "    def helper(*args, **kwargs):\n",
        "        helper.calls += 1\n",
        "        return func(*args, **kwargs)\n",
        "    helper.calls = 0\n",
        "    helper.__name__= func.__name__\n",
        "\n",
        "    return helper\n",
        "memo = {}\n",
        "@call_counter\n",
        "def levenshtein(s, t):\n",
        "    if s == \"\":\n",
        "        return len(t)\n",
        "    if t == \"\":\n",
        "        return len(s)\n",
        "    cost = 0 if s[-1] == t[-1] else 1\n",
        "       \n",
        "    i1 = (s[:-1], t)\n",
        "    if not i1 in memo:\n",
        "        memo[i1] = levenshtein(*i1)\n",
        "    i2 = (s, t[:-1])\n",
        "    if not i2 in memo:\n",
        "        memo[i2] = levenshtein(*i2)\n",
        "    i3 = (s[:-1], t[:-1])\n",
        "    if not i3 in memo:\n",
        "        memo[i3] = levenshtein(*i3)\n",
        "    res = min([memo[i1]+1, memo[i2]+1, memo[i3]+cost])\n",
        "    \n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dictionary with nearest words\n",
        "\n",
        "\n",
        "def min_dist_word(token, beamwidth = 10 ):\n",
        "    mydict = {}\n",
        "    for i in range(0,beamwidth):\n",
        "        mydict[i] = 500 \n",
        "\n",
        "    for word in vocabulary:\n",
        "        d = levenshtein(token,word)\n",
        "        key_max = max(mydict.keys(), key=(lambda k: mydict[k]))\n",
        "        if d < mydict[key_max]:\n",
        "            mydict.pop(key_max)\n",
        "            mydict[word] = d\n",
        "\n",
        "    for key in mydict:\n",
        "        mydict[key] = 1/(mydict[key] + 1)\n",
        "\n",
        "    return mydict # returns for each token, one dictionary with { word : 1/(dist +1) }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "# Spelling corrector\n",
        "\n",
        "def findWriteSentence(sentence,beamwidth=10):\n",
        "    l1 = 0.4;\n",
        "    l2 = 0.6;\n",
        "    sentence = '<s> '+sentence; \n",
        "    sentence_array = tweet_wt.tokenize(sentence)\n",
        "    \n",
        "    editdistance = [] # table with dictionaries with the 10 closest words of each word of the sequence\n",
        "    edtdistance = [min_dist_word(sentence_array[k],5) for k in range(1,len(sentence_array))]\n",
        "\n",
        "    path = [{('<s>','<s>'):0}]\n",
        "   \n",
        "    for i in range(len(edtdistance)): \n",
        "        diction = dict()\n",
        "        for (prevword, token) in path[len(path)-1]:\n",
        "            for word in edtdistance[i]:\n",
        "                LK = -l1* findBigramPropability(token, word) -l2* math.log2(edtdistance[i][word])  \n",
        "                diction[token, word] = path[len(path)-1][prevword,token] +LK\n",
        "        j = len(diction)\n",
        "        \n",
        "        while j >2:\n",
        "            key_max = max(diction.keys(), key=(lambda k: diction[k]))\n",
        "            diction.pop(key_max)\n",
        "            j = j-1\n",
        "        path.append(diction) \n",
        "\n",
        "    lastitem = path[len(path)-1];\n",
        "    key_min = min(lastitem.keys(), key=(lambda k: lastitem[k]))\n",
        "\n",
        "    prevword,lastword = key_min[0],key_min[1]\n",
        "    right_sentence = lastword\n",
        "\n",
        "    index= len(path)-2\n",
        "    while(index > 0):\n",
        "        previtem = path[index]\n",
        "        for key in previtem:\n",
        "            if (key[1] ==prevword):\n",
        "                right_sentence = key[1]+' '+right_sentence\n",
        "\n",
        "                prevword = key[0]\n",
        "        index =index-1\n",
        "    \n",
        "    return(right_sentence)\n",
        "\n",
        "initial_sentence = input('Give a sentence') #test with input sentence\n",
        "print(findWriteSentence(initial_sentence))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DP5l-YYYfkU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}